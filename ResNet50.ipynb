{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing (taken from load_images.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some intializations\n",
    "# data_dir = '/content/drive/MyDrive/CS172B/'\n",
    "data_dir = ''\n",
    "train_path = data_dir + 'train'\n",
    "test_path = data_dir + 'test'\n",
    "affectNet_train_path =  data_dir + 'Affectnet_train' \n",
    "labels = ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
    "\n",
    "def isNormalizedIMG(img):\n",
    "    \"\"\"Check whether the image has been normalized before\"\"\"\n",
    "    for i in img:\n",
    "        for j in i:\n",
    "            for k in j:\n",
    "                if k > 1: return False # has not been normalized\n",
    "                if k < 1 and k > 0: return True # has been normalized\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []; Y = []\n",
    "X_test =[]; Y_test =[]\n",
    "count = np.zeros((7))\n",
    "\n",
    "# load FER2013 training dataset\n",
    "for i in range(0,7):\n",
    "    path = train_path+'/'+labels[i]\n",
    "    for j in os.listdir(path):\n",
    "        img = plt.imread(path+'/'+j) #load images\n",
    "        RGB_img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB) #convert to RGB\n",
    "        if(isNormalizedIMG(RGB_img) == False): RGB_img = RGB_img/255 #if hasn't been normalized, normalize it\n",
    "        X.append(RGB_img)\n",
    "        Y.append(i)\n",
    "        count[i]+=1\n",
    "        \n",
    "# load affectnet training dataset\n",
    "for i in range(0,7):\n",
    "    path = affectNet_train_path+'/'+labels[i]\n",
    "    for j in os.listdir(path):\n",
    "        img = plt.imread(path+'/'+j) #load images\n",
    "        resized_img = cv2.resize(img,(48,48)) #resize to (48,48,3)\n",
    "        if(isNormalizedIMG(resized_img) == False): resized_img = resized_img/255 #if hasn't been normalized, normalize it\n",
    "        X.append(resized_img)\n",
    "        Y.append(i)\n",
    "        count[i]+=1\n",
    "\n",
    "# load FER2013 testing dataset\n",
    "for i in range(0,7):\n",
    "    path = test_path+'/'+labels[i]\n",
    "    for j in os.listdir(path):\n",
    "        img = plt.imread(path+'/'+j) #load images\n",
    "        RGB_img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB) #convert to RGB\n",
    "        if(isNormalizedIMG(RGB_img) == False): RGB_img = RGB_img/255 #If hasn't been normalized, normalize it\n",
    "        X_test.append(RGB_img)\n",
    "        Y_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "X = np.array(X); Y = np.array(Y); X_test = np.array(X_test); Y_test = np.array(Y_test)\n",
    "X_train,X_validation,Y_train,Y_validation = train_test_split(X,Y,test_size = 0.25, random_state = seed, shuffle = True, stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset():\n",
    "    \"\"\"Return np.arrays.\"\"\"\n",
    "    \"\"\"Training set, Training set labels, Validation set, Validation set labels, Testing set, Testing set labels \"\"\"\n",
    "    return X_train, Y_train, X_validation, Y_validation, X_test, Y_test\n",
    "    \n",
    "def getDatasetCount():\n",
    "    \"\"\"Return count for each labels in the dataset\"\"\"\n",
    "    return count\n",
    "\n",
    "def getLabels():\n",
    "    \"\"\"Return labels in the dataset\"\"\"\n",
    "    return labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 13:29:42.786044: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.initializers import RandomUniform, Zeros, HeNormal, HeUniform\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, Reshape, GlobalAveragePooling2D, ZeroPadding2D, Cropping2D\n",
    "from keras.applications.resnet_v2 import ResNet50V2, preprocess_input\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam, Adamax\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import os\n",
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_validation, Y_validation, X_test, Y_test = getDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet Mandatory Preprocessing\n",
    "X_train = preprocess_input(X_train)\n",
    "X_validation = preprocess_input(X_validation)\n",
    "X_test = preprocess_input(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (41160, 48, 48, 3)\n",
      "X_validation shape: (13720, 48, 48, 3)\n",
      "X_test shape: (7178, 48, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_validation shape: {X_validation.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: \n",
      "\tpooling value: avg\n",
      "\tbias_init value: <keras.initializers.initializers_v2.RandomUniform object at 0x7faee7574790>\n",
      "\tlr value: 0.008\n",
      "\toptimizer value: Adamax\n",
      "\tbatch_size value: 128\n",
      "\tactivation_function value: relu\n",
      "\tbias_regularizer value: l2\n",
      "\tdropout_rate value: 0.4\n",
      "\tloss_function value: <keras.losses.SparseCategoricalCrossentropy object at 0x7faee7474a00>\n",
      "\tepoch value: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 13:31:19.498215: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2048)             8192      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 32, 32, 2)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 3, 3, 64)          115264    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3, 3, 64)          0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 64)               0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,688,711\n",
      "Trainable params: 119,815\n",
      "Non-trainable params: 23,568,896\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.7701 - accuracy: 0.2940"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - 449s 1s/step - loss: 1.7701 - accuracy: 0.2940 - val_loss: 1.7779 - val_accuracy: 0.3093\n",
      "Epoch 2/100\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.7033 - accuracy: 0.3291"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - 349s 1s/step - loss: 1.7033 - accuracy: 0.3291 - val_loss: 1.6927 - val_accuracy: 0.3292\n",
      "Epoch 3/100\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.6816 - accuracy: 0.3336"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - 333s 1s/step - loss: 1.6816 - accuracy: 0.3336 - val_loss: 1.6675 - val_accuracy: 0.3473\n",
      "Epoch 4/100\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.6640 - accuracy: 0.3452"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - 343s 1s/step - loss: 1.6640 - accuracy: 0.3452 - val_loss: 1.6577 - val_accuracy: 0.3460\n",
      "Epoch 5/100\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.6517 - accuracy: 0.3459"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - 312s 969ms/step - loss: 1.6517 - accuracy: 0.3459 - val_loss: 1.6540 - val_accuracy: 0.3492\n",
      "Epoch 6/100\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.6392 - accuracy: 0.3577"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - 311s 967ms/step - loss: 1.6392 - accuracy: 0.3577 - val_loss: 1.6502 - val_accuracy: 0.3555\n",
      "Epoch 7/100\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.6319 - accuracy: 0.3607"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - 322s 1s/step - loss: 1.6319 - accuracy: 0.3607 - val_loss: 1.6474 - val_accuracy: 0.3482\n",
      "Epoch 8/100\n",
      "322/322 [==============================] - 288s 894ms/step - loss: 1.6247 - accuracy: 0.3608 - val_loss: 1.6565 - val_accuracy: 0.3512\n",
      "Epoch 9/100\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.6186 - accuracy: 0.3643"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - 312s 968ms/step - loss: 1.6186 - accuracy: 0.3643 - val_loss: 1.6377 - val_accuracy: 0.3582\n",
      "Epoch 10/100\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.6107 - accuracy: 0.3675"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - 317s 984ms/step - loss: 1.6107 - accuracy: 0.3675 - val_loss: 1.6273 - val_accuracy: 0.3580\n",
      "Epoch 11/100\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.6065 - accuracy: 0.3683"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - 330s 1s/step - loss: 1.6065 - accuracy: 0.3683 - val_loss: 1.6228 - val_accuracy: 0.3621\n",
      "Epoch 12/100\n",
      "322/322 [==============================] - 292s 907ms/step - loss: 1.5967 - accuracy: 0.3750 - val_loss: 1.6330 - val_accuracy: 0.3551\n",
      "Epoch 13/100\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.5931 - accuracy: 0.3759"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 54). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322/322 [==============================] - 329s 1s/step - loss: 1.5931 - accuracy: 0.3759 - val_loss: 1.6219 - val_accuracy: 0.3660\n",
      "Epoch 14/100\n",
      "322/322 [==============================] - ETA: 0s - loss: 1.5876 - accuracy: 0.3749"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 96\u001b[0m\n\u001b[1;32m     85\u001b[0m callbacks \u001b[39m=\u001b[39m [\n\u001b[1;32m     86\u001b[0m     keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\n\u001b[1;32m     87\u001b[0m         filepath \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodels/test\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m ]\n\u001b[1;32m     93\u001b[0m \u001b[39m#####################################################################\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m###                          Fit Model                            ###\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m#####################################################################\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     97\u001b[0m     X_train, Y_train, \n\u001b[1;32m     98\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_validation, Y_validation),\n\u001b[1;32m     99\u001b[0m     batch_size\u001b[39m=\u001b[39;49mhyperparam_batch_size, \n\u001b[1;32m    100\u001b[0m     epochs\u001b[39m=\u001b[39;49mhyperparam_epoch, \n\u001b[1;32m    101\u001b[0m     callbacks \u001b[39m=\u001b[39;49m callbacks\n\u001b[1;32m    102\u001b[0m )\n\u001b[1;32m    105\u001b[0m \u001b[39m#####################################################################\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m###                    Plot Training Results                      ###\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39m### Note: Modify code to store plots in a excel spreadsheet       ###\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m###         rather than plot.                                     ###\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m#####################################################################\u001b[39;00m\n\u001b[1;32m    110\u001b[0m acc \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/engine/training.py:1624\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1619\u001b[0m     val_logs \u001b[39m=\u001b[39m {\n\u001b[1;32m   1620\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1621\u001b[0m     }\n\u001b[1;32m   1622\u001b[0m     epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n\u001b[0;32m-> 1624\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_epoch_end(epoch, epoch_logs)\n\u001b[1;32m   1625\u001b[0m training_logs \u001b[39m=\u001b[39m epoch_logs\n\u001b[1;32m   1626\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/callbacks.py:448\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    446\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    447\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 448\u001b[0m     callback\u001b[39m.\u001b[39;49mon_epoch_end(epoch, logs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/callbacks.py:1463\u001b[0m, in \u001b[0;36mModelCheckpoint.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1460\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs_since_last_save \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_freq \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_model(epoch\u001b[39m=\u001b[39;49mepoch, batch\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/callbacks.py:1528\u001b[0m, in \u001b[0;36mModelCheckpoint._save_model\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave_weights(\n\u001b[1;32m   1523\u001b[0m             filepath,\n\u001b[1;32m   1524\u001b[0m             overwrite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1525\u001b[0m             options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options,\n\u001b[1;32m   1526\u001b[0m         )\n\u001b[1;32m   1527\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1528\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msave(\n\u001b[1;32m   1529\u001b[0m             filepath,\n\u001b[1;32m   1530\u001b[0m             overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1531\u001b[0m             options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_options,\n\u001b[1;32m   1532\u001b[0m         )\n\u001b[1;32m   1533\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1534\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/engine/training.py:2698\u001b[0m, in \u001b[0;36mModel.save\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   2643\u001b[0m \u001b[39m@traceback_utils\u001b[39m\u001b[39m.\u001b[39mfilter_traceback\n\u001b[1;32m   2644\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(\n\u001b[1;32m   2645\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2652\u001b[0m     save_traces\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2653\u001b[0m ):\n\u001b[1;32m   2655\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Saves the model to Tensorflow SavedModel or a single HDF5 file.\u001b[39;00m\n\u001b[1;32m   2656\u001b[0m \n\u001b[1;32m   2657\u001b[0m \u001b[39m    Please see `tf.keras.models.save_model` or the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[1;32m   2696\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2698\u001b[0m     save\u001b[39m.\u001b[39;49msave_model(\n\u001b[1;32m   2699\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2700\u001b[0m         filepath,\n\u001b[1;32m   2701\u001b[0m         overwrite,\n\u001b[1;32m   2702\u001b[0m         include_optimizer,\n\u001b[1;32m   2703\u001b[0m         save_format,\n\u001b[1;32m   2704\u001b[0m         signatures,\n\u001b[1;32m   2705\u001b[0m         options,\n\u001b[1;32m   2706\u001b[0m         save_traces,\n\u001b[1;32m   2707\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/save.py:166\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39mwith\u001b[39;00m generic_utils\u001b[39m.\u001b[39mSharedObjectSavingScope():\n\u001b[0;32m--> 166\u001b[0m         saved_model_save\u001b[39m.\u001b[39;49msave(\n\u001b[1;32m    167\u001b[0m             model,\n\u001b[1;32m    168\u001b[0m             filepath,\n\u001b[1;32m    169\u001b[0m             overwrite,\n\u001b[1;32m    170\u001b[0m             include_optimizer,\n\u001b[1;32m    171\u001b[0m             signatures,\n\u001b[1;32m    172\u001b[0m             options,\n\u001b[1;32m    173\u001b[0m             save_traces,\n\u001b[1;32m    174\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/save.py:97\u001b[0m, in \u001b[0;36msave\u001b[0;34m(model, filepath, overwrite, include_optimizer, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mwith\u001b[39;00m backend\u001b[39m.\u001b[39mdeprecated_internal_learning_phase_scope(\u001b[39m0\u001b[39m):\n\u001b[1;32m     96\u001b[0m     \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39mkeras_option_scope(save_traces):\n\u001b[0;32m---> 97\u001b[0m         saved_nodes, node_paths \u001b[39m=\u001b[39m save_lib\u001b[39m.\u001b[39;49msave_and_return_nodes(\n\u001b[1;32m     98\u001b[0m             model, filepath, signatures, options\n\u001b[1;32m     99\u001b[0m         )\n\u001b[1;32m    101\u001b[0m     \u001b[39m# Save all metadata to a separate file in the SavedModel directory.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     metadata \u001b[39m=\u001b[39m generate_keras_metadata(saved_nodes, node_paths)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/saved_model/save.py:1268\u001b[0m, in \u001b[0;36msave_and_return_nodes\u001b[0;34m(obj, export_dir, signatures, options, experimental_skip_checkpoint)\u001b[0m\n\u001b[1;32m   1264\u001b[0m saved_model \u001b[39m=\u001b[39m saved_model_pb2\u001b[39m.\u001b[39mSavedModel()\n\u001b[1;32m   1265\u001b[0m meta_graph_def \u001b[39m=\u001b[39m saved_model\u001b[39m.\u001b[39mmeta_graphs\u001b[39m.\u001b[39madd()\n\u001b[1;32m   1267\u001b[0m _, exported_graph, object_saver, asset_info, saved_nodes, node_paths \u001b[39m=\u001b[39m (\n\u001b[0;32m-> 1268\u001b[0m     _build_meta_graph(obj, signatures, options, meta_graph_def))\n\u001b[1;32m   1269\u001b[0m saved_model\u001b[39m.\u001b[39msaved_model_schema_version \u001b[39m=\u001b[39m (\n\u001b[1;32m   1270\u001b[0m     constants\u001b[39m.\u001b[39mSAVED_MODEL_SCHEMA_VERSION)\n\u001b[1;32m   1272\u001b[0m \u001b[39m# Write the checkpoint, copy assets into the assets directory, and write out\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m \u001b[39m# the SavedModel proto itself.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/saved_model/save.py:1441\u001b[0m, in \u001b[0;36m_build_meta_graph\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a MetaGraph under a save context.\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m \n\u001b[1;32m   1416\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1437\u001b[0m \u001b[39m  saveable_view.node_paths: _SaveableView paths.\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m \u001b[39mwith\u001b[39;00m save_context\u001b[39m.\u001b[39msave_context(options):\n\u001b[0;32m-> 1441\u001b[0m   \u001b[39mreturn\u001b[39;00m _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/saved_model/save.py:1384\u001b[0m, in \u001b[0;36m_build_meta_graph_impl\u001b[0;34m(obj, signatures, options, meta_graph_def)\u001b[0m\n\u001b[1;32m   1382\u001b[0m augmented_graph_view \u001b[39m=\u001b[39m _AugmentedGraphView(obj)\n\u001b[1;32m   1383\u001b[0m \u001b[39mif\u001b[39;00m signatures \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1384\u001b[0m   signatures \u001b[39m=\u001b[39m signature_serialization\u001b[39m.\u001b[39;49mfind_function_to_export(\n\u001b[1;32m   1385\u001b[0m       augmented_graph_view)\n\u001b[1;32m   1387\u001b[0m signatures, wrapped_functions \u001b[39m=\u001b[39m (\n\u001b[1;32m   1388\u001b[0m     signature_serialization\u001b[39m.\u001b[39mcanonicalize_signatures(signatures))\n\u001b[1;32m   1389\u001b[0m signature_serialization\u001b[39m.\u001b[39mvalidate_augmented_graph_view(augmented_graph_view)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/saved_model/signature_serialization.py:103\u001b[0m, in \u001b[0;36mfind_function_to_export\u001b[0;34m(saveable_view)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m# TODO(b/205014194): Discuss removing this behaviour. It can lead to WTFs when\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m# a user decides to annotate more functions with tf.function and suddenly\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m# serving that model way later in the process stops working.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m possible_signatures \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 103\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m children:\n\u001b[1;32m    104\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(child, (def_function\u001b[39m.\u001b[39mFunction, defun\u001b[39m.\u001b[39mConcreteFunction)):\n\u001b[1;32m    105\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/saved_model/save.py:175\u001b[0m, in \u001b[0;36m_AugmentedGraphView.list_children\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_children_cache:\n\u001b[1;32m    173\u001b[0m   children \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_children_cache[obj] \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 175\u001b[0m   \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m \u001b[39msuper\u001b[39;49m(_AugmentedGraphView, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlist_children(\n\u001b[1;32m    176\u001b[0m       obj,\n\u001b[1;32m    177\u001b[0m       save_type\u001b[39m=\u001b[39;49mbase\u001b[39m.\u001b[39;49mSaveType\u001b[39m.\u001b[39;49mSAVEDMODEL,\n\u001b[1;32m    178\u001b[0m       cache\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_serialization_cache):\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(child, defun\u001b[39m.\u001b[39mConcreteFunction):\n\u001b[1;32m    180\u001b[0m       child \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_uncache_variable_captures(child)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/checkpoint/graph_view.py:75\u001b[0m, in \u001b[0;36mObjectGraphView.list_children\u001b[0;34m(self, obj, save_type, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns list of all child trackables attached to obj.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m  List of all children attached to the object.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m children \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 75\u001b[0m \u001b[39mfor\u001b[39;00m name, ref \u001b[39min\u001b[39;00m \u001b[39msuper\u001b[39;49m(ObjectGraphView,\n\u001b[1;32m     76\u001b[0m                        \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mchildren(obj, save_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     77\u001b[0m   children\u001b[39m.\u001b[39mappend(base\u001b[39m.\u001b[39mTrackableReference(name, ref))\n\u001b[1;32m     79\u001b[0m \u001b[39m# GraphView objects may define children of the root object that are not\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m# actually attached, e.g. a Checkpoint object's save_counter.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/checkpoint/trackable_view.py:84\u001b[0m, in \u001b[0;36mTrackableView.children\u001b[0;34m(cls, obj, save_type, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m obj\u001b[39m.\u001b[39m_maybe_initialize_trackable()\n\u001b[1;32m     83\u001b[0m children \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 84\u001b[0m \u001b[39mfor\u001b[39;00m name, ref \u001b[39min\u001b[39;00m obj\u001b[39m.\u001b[39;49m_trackable_children(save_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     85\u001b[0m   ref \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39mconvert_to_trackable(ref, parent\u001b[39m=\u001b[39mobj)\n\u001b[1;32m     86\u001b[0m   children[name] \u001b[39m=\u001b[39m ref\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/engine/functional.py:459\u001b[0m, in \u001b[0;36mFunctional._trackable_children\u001b[0;34m(self, save_type, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_trackable_children\u001b[39m(\u001b[39mself\u001b[39m, save_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcheckpoint\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    458\u001b[0m     dependencies \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layer_checkpoint_dependencies\n\u001b[0;32m--> 459\u001b[0m     dependencies\u001b[39m.\u001b[39mupdate(\u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_trackable_children(save_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    460\u001b[0m     \u001b[39mreturn\u001b[39;00m dependencies\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/engine/training.py:3666\u001b[0m, in \u001b[0;36mModel._trackable_children\u001b[0;34m(self, save_type, **kwargs)\u001b[0m\n\u001b[1;32m   3663\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_function \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3664\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_tf_function \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 3666\u001b[0m children \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_trackable_children(save_type, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3668\u001b[0m \u001b[39mif\u001b[39;00m save_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msavedmodel\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   3669\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m train_function\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/engine/base_layer.py:3337\u001b[0m, in \u001b[0;36mLayer._trackable_children\u001b[0;34m(self, save_type, **kwargs)\u001b[0m\n\u001b[1;32m   3333\u001b[0m     cache \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   3334\u001b[0m     \u001b[39m# TODO(b/213628533): This must be called before super() to ensure\u001b[39;00m\n\u001b[1;32m   3335\u001b[0m     \u001b[39m# that any input shape changes are applied before getting the config\u001b[39;00m\n\u001b[1;32m   3336\u001b[0m     \u001b[39m# of the model.\u001b[39;00m\n\u001b[0;32m-> 3337\u001b[0m     children \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_trackable_saved_model_saver\u001b[39m.\u001b[39;49mtrackable_children(\n\u001b[1;32m   3338\u001b[0m         cache\n\u001b[1;32m   3339\u001b[0m     )\n\u001b[1;32m   3340\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3341\u001b[0m     children \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/base_serialization.py:61\u001b[0m, in \u001b[0;36mSavedModelSaver.trackable_children\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m utils\u001b[39m.\u001b[39mshould_save_traces():\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m {}\n\u001b[0;32m---> 61\u001b[0m children \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobjects_to_serialize(serialization_cache)\n\u001b[1;32m     62\u001b[0m children\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunctions_to_serialize(serialization_cache))\n\u001b[1;32m     63\u001b[0m \u001b[39mreturn\u001b[39;00m children\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/layer_serialization.py:79\u001b[0m, in \u001b[0;36mLayerSavedModelSaver.objects_to_serialize\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjects_to_serialize\u001b[39m(\u001b[39mself\u001b[39m, serialization_cache):\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_serialized_attributes(\n\u001b[1;32m     80\u001b[0m         serialization_cache\n\u001b[1;32m     81\u001b[0m     )\u001b[39m.\u001b[39mobjects_to_serialize\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/layer_serialization.py:106\u001b[0m, in \u001b[0;36mLayerSavedModelSaver._get_serialized_attributes\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    101\u001b[0m     save_impl\u001b[39m.\u001b[39mshould_skip_serialization(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m    102\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_must_restore_from_config\n\u001b[1;32m    103\u001b[0m ):\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m serialized_attr\n\u001b[0;32m--> 106\u001b[0m object_dict, function_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_serialized_attributes_internal(\n\u001b[1;32m    107\u001b[0m     serialization_cache\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    110\u001b[0m serialized_attr\u001b[39m.\u001b[39mset_and_validate_objects(object_dict)\n\u001b[1;32m    111\u001b[0m serialized_attr\u001b[39m.\u001b[39mset_and_validate_functions(function_dict)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/model_serialization.py:57\u001b[0m, in \u001b[0;36mModelSavedModelSaver._get_serialized_attributes_internal\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     53\u001b[0m     default_signature \u001b[39m=\u001b[39m save_impl\u001b[39m.\u001b[39mdefault_save_signature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m     55\u001b[0m \u001b[39m# Other than the default signature function, all other attributes match\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# with the ones serialized by Layer.\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m objects, functions \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_get_serialized_attributes_internal(\n\u001b[1;32m     58\u001b[0m     serialization_cache\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     60\u001b[0m functions[\u001b[39m\"\u001b[39m\u001b[39m_default_save_signature\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m default_signature\n\u001b[1;32m     61\u001b[0m \u001b[39mreturn\u001b[39;00m objects, functions\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/layer_serialization.py:117\u001b[0m, in \u001b[0;36mLayerSavedModelSaver._get_serialized_attributes_internal\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns dictionary of serialized attributes.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m objects \u001b[39m=\u001b[39m save_impl\u001b[39m.\u001b[39mwrap_layer_objects(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, serialization_cache)\n\u001b[0;32m--> 117\u001b[0m functions \u001b[39m=\u001b[39m save_impl\u001b[39m.\u001b[39;49mwrap_layer_functions(\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj, serialization_cache\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    120\u001b[0m \u001b[39m# Attribute validator requires that the default save signature is added\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m# to function dict, even if the value is None.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m functions[\u001b[39m\"\u001b[39m\u001b[39m_default_save_signature\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/save_impl.py:168\u001b[0m, in \u001b[0;36mwrap_layer_functions\u001b[0;34m(layer, serialization_cache)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    162\u001b[0m         fn_name: \u001b[39mgetattr\u001b[39m(layer\u001b[39m.\u001b[39mkeras_api, fn_name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m         \u001b[39mfor\u001b[39;00m fn_name \u001b[39min\u001b[39;00m serialized_attributes\u001b[39m.\u001b[39mLayerAttributes\u001b[39m.\u001b[39mall_functions\n\u001b[1;32m    164\u001b[0m     }\n\u001b[1;32m    166\u001b[0m \u001b[39m# Reset the losses of the layer and its children. The call function in each\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m# child layer is replaced with tf.functions.\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m original_fns \u001b[39m=\u001b[39m _replace_child_layer_functions(layer, serialization_cache)\n\u001b[1;32m    169\u001b[0m original_losses \u001b[39m=\u001b[39m _reset_layer_losses(layer)\n\u001b[1;32m    171\u001b[0m \u001b[39m# Wrap all the layer call and activity regularizer functions.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \n\u001b[1;32m    173\u001b[0m \u001b[39m# Use LayerCallCollection to ensure that all layer call functions (__call__,\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39m# call with losses) are traced with the same inputs.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/save_impl.py:307\u001b[0m, in \u001b[0;36m_replace_child_layer_functions\u001b[0;34m(layer, serialization_cache)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[39mif\u001b[39;00m child_layer \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m serialization_cache[constants\u001b[39m.\u001b[39mKERAS_CACHE_KEY]:\n\u001b[0;32m--> 307\u001b[0m     serialized_functions \u001b[39m=\u001b[39m child_layer\u001b[39m.\u001b[39;49m_trackable_saved_model_saver\u001b[39m.\u001b[39;49m_get_serialized_attributes(  \u001b[39m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    308\u001b[0m         serialization_cache\n\u001b[1;32m    309\u001b[0m     )\u001b[39m.\u001b[39mfunctions\n\u001b[1;32m    310\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     serialized_functions \u001b[39m=\u001b[39m serialization_cache[\n\u001b[1;32m    312\u001b[0m         constants\u001b[39m.\u001b[39mKERAS_CACHE_KEY\n\u001b[1;32m    313\u001b[0m     ][child_layer]\u001b[39m.\u001b[39mfunctions\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/layer_serialization.py:106\u001b[0m, in \u001b[0;36mLayerSavedModelSaver._get_serialized_attributes\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    101\u001b[0m     save_impl\u001b[39m.\u001b[39mshould_skip_serialization(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m    102\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_must_restore_from_config\n\u001b[1;32m    103\u001b[0m ):\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m serialized_attr\n\u001b[0;32m--> 106\u001b[0m object_dict, function_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_serialized_attributes_internal(\n\u001b[1;32m    107\u001b[0m     serialization_cache\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    110\u001b[0m serialized_attr\u001b[39m.\u001b[39mset_and_validate_objects(object_dict)\n\u001b[1;32m    111\u001b[0m serialized_attr\u001b[39m.\u001b[39mset_and_validate_functions(function_dict)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/model_serialization.py:57\u001b[0m, in \u001b[0;36mModelSavedModelSaver._get_serialized_attributes_internal\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m     53\u001b[0m     default_signature \u001b[39m=\u001b[39m save_impl\u001b[39m.\u001b[39mdefault_save_signature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m     55\u001b[0m \u001b[39m# Other than the default signature function, all other attributes match\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# with the ones serialized by Layer.\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m objects, functions \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_get_serialized_attributes_internal(\n\u001b[1;32m     58\u001b[0m     serialization_cache\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     60\u001b[0m functions[\u001b[39m\"\u001b[39m\u001b[39m_default_save_signature\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m default_signature\n\u001b[1;32m     61\u001b[0m \u001b[39mreturn\u001b[39;00m objects, functions\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/layer_serialization.py:117\u001b[0m, in \u001b[0;36mLayerSavedModelSaver._get_serialized_attributes_internal\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns dictionary of serialized attributes.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m objects \u001b[39m=\u001b[39m save_impl\u001b[39m.\u001b[39mwrap_layer_objects(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, serialization_cache)\n\u001b[0;32m--> 117\u001b[0m functions \u001b[39m=\u001b[39m save_impl\u001b[39m.\u001b[39;49mwrap_layer_functions(\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj, serialization_cache\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    120\u001b[0m \u001b[39m# Attribute validator requires that the default save signature is added\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m# to function dict, even if the value is None.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m functions[\u001b[39m\"\u001b[39m\u001b[39m_default_save_signature\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/save_impl.py:168\u001b[0m, in \u001b[0;36mwrap_layer_functions\u001b[0;34m(layer, serialization_cache)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    162\u001b[0m         fn_name: \u001b[39mgetattr\u001b[39m(layer\u001b[39m.\u001b[39mkeras_api, fn_name, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m         \u001b[39mfor\u001b[39;00m fn_name \u001b[39min\u001b[39;00m serialized_attributes\u001b[39m.\u001b[39mLayerAttributes\u001b[39m.\u001b[39mall_functions\n\u001b[1;32m    164\u001b[0m     }\n\u001b[1;32m    166\u001b[0m \u001b[39m# Reset the losses of the layer and its children. The call function in each\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m# child layer is replaced with tf.functions.\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m original_fns \u001b[39m=\u001b[39m _replace_child_layer_functions(layer, serialization_cache)\n\u001b[1;32m    169\u001b[0m original_losses \u001b[39m=\u001b[39m _reset_layer_losses(layer)\n\u001b[1;32m    171\u001b[0m \u001b[39m# Wrap all the layer call and activity regularizer functions.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \n\u001b[1;32m    173\u001b[0m \u001b[39m# Use LayerCallCollection to ensure that all layer call functions (__call__,\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39m# call with losses) are traced with the same inputs.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/save_impl.py:307\u001b[0m, in \u001b[0;36m_replace_child_layer_functions\u001b[0;34m(layer, serialization_cache)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[39mif\u001b[39;00m child_layer \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m serialization_cache[constants\u001b[39m.\u001b[39mKERAS_CACHE_KEY]:\n\u001b[0;32m--> 307\u001b[0m     serialized_functions \u001b[39m=\u001b[39m child_layer\u001b[39m.\u001b[39;49m_trackable_saved_model_saver\u001b[39m.\u001b[39;49m_get_serialized_attributes(  \u001b[39m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    308\u001b[0m         serialization_cache\n\u001b[1;32m    309\u001b[0m     )\u001b[39m.\u001b[39mfunctions\n\u001b[1;32m    310\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     serialized_functions \u001b[39m=\u001b[39m serialization_cache[\n\u001b[1;32m    312\u001b[0m         constants\u001b[39m.\u001b[39mKERAS_CACHE_KEY\n\u001b[1;32m    313\u001b[0m     ][child_layer]\u001b[39m.\u001b[39mfunctions\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/layer_serialization.py:106\u001b[0m, in \u001b[0;36mLayerSavedModelSaver._get_serialized_attributes\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    101\u001b[0m     save_impl\u001b[39m.\u001b[39mshould_skip_serialization(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[1;32m    102\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_must_restore_from_config\n\u001b[1;32m    103\u001b[0m ):\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m serialized_attr\n\u001b[0;32m--> 106\u001b[0m object_dict, function_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_serialized_attributes_internal(\n\u001b[1;32m    107\u001b[0m     serialization_cache\n\u001b[1;32m    108\u001b[0m )\n\u001b[1;32m    110\u001b[0m serialized_attr\u001b[39m.\u001b[39mset_and_validate_objects(object_dict)\n\u001b[1;32m    111\u001b[0m serialized_attr\u001b[39m.\u001b[39mset_and_validate_functions(function_dict)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/layer_serialization.py:117\u001b[0m, in \u001b[0;36mLayerSavedModelSaver._get_serialized_attributes_internal\u001b[0;34m(self, serialization_cache)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns dictionary of serialized attributes.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m objects \u001b[39m=\u001b[39m save_impl\u001b[39m.\u001b[39mwrap_layer_objects(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, serialization_cache)\n\u001b[0;32m--> 117\u001b[0m functions \u001b[39m=\u001b[39m save_impl\u001b[39m.\u001b[39;49mwrap_layer_functions(\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj, serialization_cache\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    120\u001b[0m \u001b[39m# Attribute validator requires that the default save signature is added\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[39m# to function dict, even if the value is None.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m functions[\u001b[39m\"\u001b[39m\u001b[39m_default_save_signature\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/save_impl.py:218\u001b[0m, in \u001b[0;36mwrap_layer_functions\u001b[0;34m(layer, serialization_cache)\u001b[0m\n\u001b[1;32m    213\u001b[0m     fns[\u001b[39m\"\u001b[39m\u001b[39mcall_and_return_all_conditional_losses\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m call_fn_with_losses\n\u001b[1;32m    215\u001b[0m \u001b[39m# Manually trigger traces before restoring the overwritten functions. The\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39m# functions are traced within the layer call context to ensure that layer\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m# functions (e.g. add_loss) behave as though running in graph mode.\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m \u001b[39mwith\u001b[39;00m tracing_scope():\n\u001b[1;32m    219\u001b[0m     call_collection\u001b[39m.\u001b[39mtrace_with_input_signature()\n\u001b[1;32m    220\u001b[0m     \u001b[39mwith\u001b[39;00m base_layer_utils\u001b[39m.\u001b[39mcall_context()\u001b[39m.\u001b[39menter(\n\u001b[1;32m    221\u001b[0m         layer, inputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, build_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, saving\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     ):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/contextlib.py:142\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    143\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/keras/saving/saved_model/save_impl.py:392\u001b[0m, in \u001b[0;36mtracing_scope\u001b[0;34m()\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39mif\u001b[39;00m training \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[39mwith\u001b[39;00m backend\u001b[39m.\u001b[39mdeprecated_internal_learning_phase_scope(training):\n\u001b[0;32m--> 392\u001b[0m         fn\u001b[39m.\u001b[39;49mget_concrete_function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    393\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    394\u001b[0m     fn\u001b[39m.\u001b[39mget_concrete_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:1239\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1238\u001b[0m   \u001b[39m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1239\u001b[0m   concrete \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1240\u001b[0m   concrete\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:1230\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn\u001b[39m.\u001b[39m_get_concrete_function_garbage_collected(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m       \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1227\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1228\u001b[0m   \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m   \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m-> 1230\u001b[0m   concrete \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m   1231\u001b[0m       \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1232\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[1;32m   1233\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1234\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2576\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2574\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m-> 2576\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m   2577\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m   2578\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m   2579\u001b[0m       graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2760\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2758\u001b[0m   \u001b[39m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[1;32m   2759\u001b[0m   args, kwargs \u001b[39m=\u001b[39m placeholder_dict[\u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m-> 2760\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[1;32m   2762\u001b[0m graph_capture_container \u001b[39m=\u001b[39m graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_capture_func_lib  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   2763\u001b[0m \u001b[39m# Maintain the list of all captures\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2669\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[1;32m   2666\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   2667\u001b[0m ]\n\u001b[1;32m   2668\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[0;32m-> 2669\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[1;32m   2670\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m   2671\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m   2672\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m   2673\u001b[0m         args,\n\u001b[1;32m   2674\u001b[0m         kwargs,\n\u001b[1;32m   2675\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[1;32m   2676\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m   2677\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m   2678\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m   2679\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[1;32m   2680\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_function_attributes,\n\u001b[1;32m   2681\u001b[0m     spec\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_spec,\n\u001b[1;32m   2682\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;49;00m\n\u001b[1;32m   2683\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;49;00m\n\u001b[1;32m   2684\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;49;00m\n\u001b[1;32m   2685\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;49;00m\n\u001b[1;32m   2686\u001b[0m     shared_func_graph\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   2687\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1494\u001b[0m, in \u001b[0;36mConcreteFunction.__init__\u001b[0;34m(self, func_graph, attrs, shared_func_graph, spec)\u001b[0m\n\u001b[1;32m   1488\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_garbage_collector \u001b[39m=\u001b[39m ConcreteFunctionGarbageCollector(func_graph)\n\u001b[1;32m   1490\u001b[0m \u001b[39m# Pairs of forward and backward functions used for computing gradients.\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m \u001b[39m# These each get a reference to the FuncGraph deleter since they use the\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[39m# FuncGraph directly.\u001b[39;00m\n\u001b[0;32m-> 1494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_delayed_rewrite_functions \u001b[39m=\u001b[39m _DelayedRewriteGradientFunctions(\n\u001b[1;32m   1495\u001b[0m     func_graph, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attrs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_garbage_collector)\n\u001b[1;32m   1496\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_first_order_tape_functions \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1497\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_higher_order_tape_functions \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/eager/function.py:584\u001b[0m, in \u001b[0;36m_DelayedRewriteGradientFunctions.__init__\u001b[0;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_function_pairs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func_graph \u001b[39m=\u001b[39m func_graph\n\u001b[0;32m--> 584\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_function \u001b[39m=\u001b[39m _EagerDefinedFunction(\n\u001b[1;32m    585\u001b[0m     _inference_name(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func_graph\u001b[39m.\u001b[39;49mname), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func_graph,\n\u001b[1;32m    586\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func_graph\u001b[39m.\u001b[39;49minputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func_graph\u001b[39m.\u001b[39;49moutputs, attrs)\n\u001b[1;32m    587\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attrs \u001b[39m=\u001b[39m attrs\n\u001b[1;32m    588\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/eager/function.py:391\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.__init__\u001b[0;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m    390\u001b[0m   context\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 391\u001b[0m   context\u001b[39m.\u001b[39;49madd_function(fn)\n\u001b[1;32m    392\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_deleter \u001b[39m=\u001b[39m _EagerDefinedFunctionDeleter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m    393\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_registered_on_context \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/eager/context.py:2735\u001b[0m, in \u001b[0;36madd_function\u001b[0;34m(fdef)\u001b[0m\n\u001b[1;32m   2733\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_function\u001b[39m(fdef):\n\u001b[1;32m   2734\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Add a function definition to the context.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2735\u001b[0m   context()\u001b[39m.\u001b[39;49madd_function(fdef)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-172b/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1311\u001b[0m, in \u001b[0;36mContext.add_function\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Add a function definition to the context.\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \n\u001b[1;32m   1304\u001b[0m \u001b[39mOnce added, the function (identified by its name) can be executed like any\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[39m  fn: A wrapped TF_Function (returned from TF_GraphToFunction_wrapper).\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m-> 1311\u001b[0m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_ContextAddFunction(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle, fn)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "###            ONE-RUN Hyperparameters to Be Tested               ###\n",
    "#####################################################################\n",
    "# hyperparameters\n",
    "# Note: for optimizer, just write if statement or create simple function\n",
    "#           to create the optimizer object based on str value\n",
    "hyperparam_pooling = 'avg'\n",
    "hyperparam_bias_init = RandomUniform(minval=-0.01, maxval=0.01, seed=seed)\n",
    "hyperparam_lr = 0.008\n",
    "hyperparam_optimizer = 'Adamax'\n",
    "hyperparam_batch_size = 128\n",
    "hyperparam_activation_function = 'relu'\n",
    "hyperparam_bias_regularizer = 'l2'\n",
    "hyperparam_dropout_rate = 0.4   # if using dropout layer(s)\n",
    "hyperparam_loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "hyperparam_epoch = 100\n",
    "\n",
    "hyperparameters = {\n",
    "  # note: avg pooling returns a 2D tensor...idk wtf that is but probs need to resize somewhere\n",
    "  'pooling': hyperparam_pooling,\n",
    "  'bias_init': hyperparam_bias_init,\n",
    "  'lr': hyperparam_lr,\n",
    "  'optimizer': hyperparam_optimizer,\n",
    "  'batch_size': hyperparam_batch_size,\n",
    "  'activation_function': hyperparam_activation_function,\n",
    "  'bias_regularizer': hyperparam_bias_regularizer,\n",
    "  'dropout_rate': hyperparam_dropout_rate,\n",
    "  'loss_function': hyperparam_loss_function,\n",
    "  'epoch': hyperparam_epoch,\n",
    "}\n",
    "\n",
    "print(f'MODEL TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: ')\n",
    "for key, value in hyperparameters.items():\n",
    "  print(f'\\t{key} value: {value}')\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "###                    Load ResNet base_model                     ###\n",
    "#####################################################################\n",
    "base_model = ResNet50V2(include_top=False, \n",
    "                        weights='imagenet', \n",
    "                        input_shape=(48, 48, 3), \n",
    "                        pooling=hyperparam_pooling)\n",
    "\n",
    "# freeze weights of base_model - note: this works because the 'trainable' attribute is recursive \n",
    "# (i.e. bc we freeze the base model, every layer in the model is frozen)\n",
    "base_model.trainable = False\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "###                         Create Model                          ###\n",
    "#####################################################################\n",
    "# trying kernel_size=(7, 7); worked kinda, overfitting now, training data saw increase but not validation\n",
    "# trying kernel_size=(30, 30); same thing, overfitting, but better\n",
    "# trying dropout_rate=0.4\n",
    "# trying learning_rate = 0.005 (instead of 0.001); worked better kinda\n",
    "# trying GlobalAveragePooling2D() (instead of MaxPool2D() + Dense()); didn't work as well\n",
    "# trying learning_rate = 0.008 (instead of 0.005); didn't work better\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    # Flatten(),\n",
    "    BatchNormalization(),\n",
    "    Reshape(target_shape=(32, 32, 2)),\n",
    "    Conv2D(64, kernel_size=(30, 30), activation=hyperparam_activation_function),\n",
    "    Dropout(rate=0.4),\n",
    "    # MaxPool2D(pool_size=(2, 2)),\n",
    "    GlobalAveragePooling2D(),\n",
    "    Flatten(),\n",
    "    # Dense(256, activation=hyperparam_activation_function),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "#####################################################################\n",
    "###                        Compile Model                          ###\n",
    "#####################################################################\n",
    "model.compile(\n",
    "    loss = hyperparam_loss_function, \n",
    "    optimizer= Adamax(learning_rate=hyperparam_lr) if hyperparam_optimizer == 'Adamax' else Adam(learning_rate=hyperparam_lr), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath = 'models/test', \n",
    "        save_freq = 'epoch',\n",
    "        save_best_only = True\n",
    "    )\n",
    "]\n",
    "\n",
    "#####################################################################\n",
    "###                          Fit Model                            ###\n",
    "#####################################################################\n",
    "history = model.fit(\n",
    "    X_train, Y_train, \n",
    "    validation_data=(X_validation, Y_validation),\n",
    "    batch_size=hyperparam_batch_size, \n",
    "    epochs=hyperparam_epoch, \n",
    "    callbacks = callbacks\n",
    ")\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "###                    Plot Training Results                      ###\n",
    "### Note: Modify code to store plots in a excel spreadsheet       ###\n",
    "###         rather than plot.                                     ###\n",
    "#####################################################################\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(range(hyperparam_epoch), acc, label='Training Accuracy', color='red')\n",
    "plt.plot(range(hyperparam_epoch), val_acc, label='Validation Accuracy', color='blue')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(range(hyperparam_epoch), loss, label='Training Loss', color='red')\n",
    "plt.plot(range(hyperparam_epoch), val_loss, label='Validation Loss', color='blue')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "###              Best Model Prediction on Test Data               ###\n",
    "### Note: write best model's prediction in new 'results.csv' file ###\n",
    "#####################################################################\n",
    "del(model)\n",
    "model = load_model('models/test')\n",
    "prediction = model.predict(X_test) # for each datapoint, gives a (1,7) vector contains probabilities\n",
    "\n",
    "Y_prediction = np.argmax(prediction,axis = 1) # get the index(label) of highest probability for each \n",
    "prediction_accuracy = np.mean(Y_prediction==Y_test)\n",
    "print(f\"The accuracy on test data is: {prediction_accuracy*100:0.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = ImageFont.load_default()\n",
    "visualkeras.layered_view(model, legend=True, font=font, to_file='visual.png')  # font is optional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 32, 32, 2)         0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 30, 30, 64)        1216      \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_30 (MaxPoolin  (None, 14, 14, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_31 (MaxPoolin  (None, 6, 6, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               2359552   \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,296,391\n",
      "Trainable params: 2,731,591\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.6313 - accuracy: 0.3576"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 1059s 1s/step - loss: 1.6313 - accuracy: 0.3576 - val_loss: 1.6577 - val_accuracy: 0.3454\n",
      "Epoch 2/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.6273 - accuracy: 0.3588"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 633s 940ms/step - loss: 1.6273 - accuracy: 0.3588 - val_loss: 1.6576 - val_accuracy: 0.3455\n",
      "Epoch 3/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.6259 - accuracy: 0.3594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 387s 575ms/step - loss: 1.6259 - accuracy: 0.3594 - val_loss: 1.6561 - val_accuracy: 0.3482\n",
      "Epoch 4/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.6248 - accuracy: 0.3610"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 368s 547ms/step - loss: 1.6248 - accuracy: 0.3610 - val_loss: 1.6559 - val_accuracy: 0.3478\n",
      "Epoch 5/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.6239 - accuracy: 0.3614"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 356s 529ms/step - loss: 1.6239 - accuracy: 0.3614 - val_loss: 1.6553 - val_accuracy: 0.3494\n",
      "Epoch 6/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.6233 - accuracy: 0.3611"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 362s 538ms/step - loss: 1.6233 - accuracy: 0.3611 - val_loss: 1.6549 - val_accuracy: 0.3482\n",
      "Epoch 7/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.6226 - accuracy: 0.3609"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 354s 526ms/step - loss: 1.6226 - accuracy: 0.3609 - val_loss: 1.6546 - val_accuracy: 0.3488\n",
      "Epoch 8/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.6220 - accuracy: 0.3608"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 355s 527ms/step - loss: 1.6220 - accuracy: 0.3608 - val_loss: 1.6544 - val_accuracy: 0.3489\n",
      "Epoch 9/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.6215 - accuracy: 0.3618"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 357s 530ms/step - loss: 1.6215 - accuracy: 0.3618 - val_loss: 1.6539 - val_accuracy: 0.3499\n",
      "Epoch 10/10\n",
      "673/673 [==============================] - 336s 500ms/step - loss: 1.6210 - accuracy: 0.3619 - val_loss: 1.6540 - val_accuracy: 0.3503\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 56s 245ms/step\n",
      "The accuracy on test data is: 35.093%\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
    "# since we passed `training=False` when calling it. This means that\n",
    "# the batchnorm layers will not update their batch statistics.\n",
    "# This prevents the batchnorm layers from undoing all the training\n",
    "# we've done so far.\n",
    "base_model.trainable = True\n",
    "\n",
    "model.compile(\n",
    "    loss = hyperparam_loss_function, \n",
    "    optimizer= Adamax(learning_rate=1e-5) if hyperparam_optimizer == 'Adamax' else Adam(learning_rate=1e-5), \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "#####################################################################\n",
    "###                          Fit Model                            ###\n",
    "#####################################################################\n",
    "history = model.fit(\n",
    "    X_train, Y_train, \n",
    "    validation_data=(X_validation, Y_validation),\n",
    "    batch_size=hyperparam_batch_size, \n",
    "    epochs=hyperparam_epoch, \n",
    "    callbacks = callbacks\n",
    ")\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "###                    Plot Training Results                      ###\n",
    "### Note: Modify code to store plots in a excel spreadsheet       ###\n",
    "###         rather than plot.                                     ###\n",
    "#####################################################################\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(range(hyperparam_epoch), acc, label='Training Accuracy', color='red')\n",
    "plt.plot(range(hyperparam_epoch), val_acc, label='Validation Accuracy', color='blue')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(range(hyperparam_epoch), loss, label='Training Loss', color='red')\n",
    "plt.plot(range(hyperparam_epoch), val_loss, label='Validation Loss', color='blue')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "###              Best Model Prediction on Test Data               ###\n",
    "### Note: write best model's prediction in new 'results.csv' file ###\n",
    "#####################################################################\n",
    "del(model)\n",
    "model = load_model('models/test')\n",
    "prediction = model.predict(X_test) # for each datapoint, gives a (1,7) vector contains probabilities\n",
    "\n",
    "Y_prediction = np.argmax(prediction,axis = 1) # get the index(label) of highest probability for each \n",
    "prediction_accuracy = np.mean(Y_prediction==Y_test)\n",
    "print(f\"The accuracy on test data is: {prediction_accuracy*100:0.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualkeras.layered_view(model, legend=True, font=font, to_file='visual_post_unfreeze.png')  # font is optional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: 4 TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: \n",
      "hyperparameter currently being tested: bias_init [<keras.initializers.initializers_v2.Zeros object at 0x7f934f2048e0>]\n",
      "\tpooling value: avg\n",
      "\tbias_init value: <keras.initializers.initializers_v2.Zeros object at 0x7f934f2048e0>\n",
      "\tlr value: 0.01\n",
      "\toptimizer value: Adamax\n",
      "\tbatch_size value: 32\n",
      "\tactivation_function value: relu\n",
      "\tbias_regularizer value: l2\n",
      "\tdropout_rate value: 0.4\n",
      "\tloss_function value: <keras.losses.SparseCategoricalCrossentropy object at 0x7f934f204880>\n",
      "\tepoch value: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-04 01:38:59.995591: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense-1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense-2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense-3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense-4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,517,255\n",
      "Trainable params: 6,952,455\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.9426 - accuracy: 0.2478"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 540s 767ms/step - loss: 1.9426 - accuracy: 0.2478 - val_loss: 1.8070 - val_accuracy: 0.2449\n",
      "Epoch 2/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7947 - accuracy: 0.2610"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 505s 751ms/step - loss: 1.7947 - accuracy: 0.2610 - val_loss: 1.7815 - val_accuracy: 0.2810\n",
      "Epoch 3/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7826 - accuracy: 0.2676"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 491s 730ms/step - loss: 1.7826 - accuracy: 0.2676 - val_loss: 1.7796 - val_accuracy: 0.2731\n",
      "Epoch 4/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7728 - accuracy: 0.2721"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 549s 816ms/step - loss: 1.7728 - accuracy: 0.2721 - val_loss: 1.7533 - val_accuracy: 0.2813\n",
      "Epoch 5/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7596 - accuracy: 0.2815"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 455s 675ms/step - loss: 1.7596 - accuracy: 0.2815 - val_loss: 1.7420 - val_accuracy: 0.2957\n",
      "Epoch 6/10\n",
      "673/673 [==============================] - 437s 650ms/step - loss: 1.7499 - accuracy: 0.2843 - val_loss: 1.7608 - val_accuracy: 0.2769\n",
      "Epoch 7/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7378 - accuracy: 0.2896"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 522s 776ms/step - loss: 1.7378 - accuracy: 0.2896 - val_loss: 1.7331 - val_accuracy: 0.2844\n",
      "Epoch 8/10\n",
      "673/673 [==============================] - 457s 679ms/step - loss: 1.7299 - accuracy: 0.2943 - val_loss: 1.7425 - val_accuracy: 0.2946\n",
      "Epoch 9/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7284 - accuracy: 0.2945"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 516s 767ms/step - loss: 1.7284 - accuracy: 0.2945 - val_loss: 1.7131 - val_accuracy: 0.3132\n",
      "Epoch 10/10\n",
      "673/673 [==============================] - 465s 690ms/step - loss: 1.7197 - accuracy: 0.3019 - val_loss: 1.7271 - val_accuracy: 0.2979\n",
      "225/225 [==============================] - 72s 311ms/step\n",
      "The accuracy on test data is: 31.583%\n",
      "\n",
      "\n",
      "\n",
      "MODEL: 5 TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: \n",
      "hyperparameter currently being tested: bias_init [<keras.initializers.initializers_v2.HeNormal object at 0x7f934f204820>]\n",
      "\tpooling value: avg\n",
      "\tbias_init value: <keras.initializers.initializers_v2.HeNormal object at 0x7f934f204820>\n",
      "\tlr value: 0.01\n",
      "\toptimizer value: Adamax\n",
      "\tbatch_size value: 32\n",
      "\tactivation_function value: relu\n",
      "\tbias_regularizer value: l2\n",
      "\tdropout_rate value: 0.4\n",
      "\tloss_function value: <keras.losses.SparseCategoricalCrossentropy object at 0x7f934f204880>\n",
      "\tepoch value: 10\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense-1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense-2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense-3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense-4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,517,255\n",
      "Trainable params: 6,952,455\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.9512 - accuracy: 0.2499"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 623s 908ms/step - loss: 1.9512 - accuracy: 0.2499 - val_loss: 1.8266 - val_accuracy: 0.2456\n",
      "Epoch 2/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.8037 - accuracy: 0.2519"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 614s 912ms/step - loss: 1.8037 - accuracy: 0.2519 - val_loss: 1.7868 - val_accuracy: 0.2514\n",
      "Epoch 3/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7830 - accuracy: 0.2628"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 636s 946ms/step - loss: 1.7830 - accuracy: 0.2628 - val_loss: 1.7737 - val_accuracy: 0.2596\n",
      "Epoch 4/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7663 - accuracy: 0.2673"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 517s 768ms/step - loss: 1.7663 - accuracy: 0.2673 - val_loss: 1.7481 - val_accuracy: 0.2862\n",
      "Epoch 5/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7500 - accuracy: 0.2816"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 518s 771ms/step - loss: 1.7500 - accuracy: 0.2816 - val_loss: 1.7283 - val_accuracy: 0.2922\n",
      "Epoch 6/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7390 - accuracy: 0.2908"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 538s 800ms/step - loss: 1.7390 - accuracy: 0.2908 - val_loss: 1.7199 - val_accuracy: 0.3065\n",
      "Epoch 7/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7391 - accuracy: 0.2870"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 532s 791ms/step - loss: 1.7391 - accuracy: 0.2870 - val_loss: 1.7150 - val_accuracy: 0.3131\n",
      "Epoch 8/10\n",
      "673/673 [==============================] - 425s 631ms/step - loss: 1.7248 - accuracy: 0.2999 - val_loss: 1.7332 - val_accuracy: 0.2950\n",
      "Epoch 9/10\n",
      "673/673 [==============================] - 475s 707ms/step - loss: 1.7205 - accuracy: 0.3008 - val_loss: 1.7213 - val_accuracy: 0.3120\n",
      "Epoch 10/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7222 - accuracy: 0.3006"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 1102s 2s/step - loss: 1.7222 - accuracy: 0.3006 - val_loss: 1.7040 - val_accuracy: 0.3127\n",
      "225/225 [==============================] - 646s 3s/step\n",
      "The accuracy on test data is: 31.806%\n",
      "\n",
      "\n",
      "\n",
      "MODEL: 6 TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: \n",
      "hyperparameter currently being tested: bias_init [<keras.initializers.initializers_v2.HeUniform object at 0x7f934f204760>]\n",
      "\tpooling value: avg\n",
      "\tbias_init value: <keras.initializers.initializers_v2.HeUniform object at 0x7f934f204760>\n",
      "\tlr value: 0.01\n",
      "\toptimizer value: Adamax\n",
      "\tbatch_size value: 32\n",
      "\tactivation_function value: relu\n",
      "\tbias_regularizer value: l2\n",
      "\tdropout_rate value: 0.4\n",
      "\tloss_function value: <keras.losses.SparseCategoricalCrossentropy object at 0x7f934f204880>\n",
      "\tepoch value: 10\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense-1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense-2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense-3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense-4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,517,255\n",
      "Trainable params: 6,952,455\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.9333 - accuracy: 0.2479"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 590s 797ms/step - loss: 1.9333 - accuracy: 0.2479 - val_loss: 1.8135 - val_accuracy: 0.2455\n",
      "Epoch 2/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7999 - accuracy: 0.2557"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 523s 778ms/step - loss: 1.7999 - accuracy: 0.2557 - val_loss: 1.7658 - val_accuracy: 0.2709\n",
      "Epoch 3/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7778 - accuracy: 0.2680"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 471s 699ms/step - loss: 1.7778 - accuracy: 0.2680 - val_loss: 1.7538 - val_accuracy: 0.2810\n",
      "Epoch 4/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7693 - accuracy: 0.2707"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 458s 680ms/step - loss: 1.7693 - accuracy: 0.2707 - val_loss: 1.7504 - val_accuracy: 0.2852\n",
      "Epoch 5/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7628 - accuracy: 0.2771"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 382s 568ms/step - loss: 1.7628 - accuracy: 0.2771 - val_loss: 1.7459 - val_accuracy: 0.2842\n",
      "Epoch 6/10\n",
      "673/673 [==============================] - 353s 525ms/step - loss: 1.7533 - accuracy: 0.2802 - val_loss: 1.7461 - val_accuracy: 0.2911\n",
      "Epoch 7/10\n",
      "673/673 [==============================] - 328s 487ms/step - loss: 1.7393 - accuracy: 0.2903 - val_loss: 1.7544 - val_accuracy: 0.2912\n",
      "Epoch 8/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7470 - accuracy: 0.2836"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 382s 568ms/step - loss: 1.7470 - accuracy: 0.2836 - val_loss: 1.7264 - val_accuracy: 0.3047\n",
      "Epoch 9/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7354 - accuracy: 0.2938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 383s 569ms/step - loss: 1.7354 - accuracy: 0.2938 - val_loss: 1.7176 - val_accuracy: 0.3127\n",
      "Epoch 10/10\n",
      "673/673 [==============================] - 358s 533ms/step - loss: 1.7302 - accuracy: 0.2968 - val_loss: 1.7203 - val_accuracy: 0.3187\n",
      "225/225 [==============================] - 65s 275ms/step\n",
      "The accuracy on test data is: 31.025%\n",
      "\n",
      "\n",
      "\n",
      "MODEL: 7 TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: \n",
      "hyperparameter currently being tested: lr [0.001]\n",
      "\tpooling value: avg\n",
      "\tbias_init value: <keras.initializers.initializers_v2.RandomUniform object at 0x7f934f204850>\n",
      "\tlr value: 0.001\n",
      "\toptimizer value: Adamax\n",
      "\tbatch_size value: 32\n",
      "\tactivation_function value: relu\n",
      "\tbias_regularizer value: l2\n",
      "\tdropout_rate value: 0.4\n",
      "\tloss_function value: <keras.losses.SparseCategoricalCrossentropy object at 0x7f934f204880>\n",
      "\tepoch value: 10\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense-1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense-2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense-3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense-4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,517,255\n",
      "Trainable params: 6,952,455\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.8165 - accuracy: 0.2439"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 391s 574ms/step - loss: 1.8165 - accuracy: 0.2439 - val_loss: 1.7989 - val_accuracy: 0.2551\n",
      "Epoch 2/10\n",
      "673/673 [==============================] - 344s 512ms/step - loss: 1.7879 - accuracy: 0.2584 - val_loss: 1.8026 - val_accuracy: 0.2430\n",
      "Epoch 3/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7557 - accuracy: 0.2780"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 414s 614ms/step - loss: 1.7557 - accuracy: 0.2780 - val_loss: 1.7876 - val_accuracy: 0.2433\n",
      "Epoch 4/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7331 - accuracy: 0.2957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 367s 545ms/step - loss: 1.7331 - accuracy: 0.2957 - val_loss: 1.7282 - val_accuracy: 0.2920\n",
      "Epoch 5/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7165 - accuracy: 0.3066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 413s 614ms/step - loss: 1.7165 - accuracy: 0.3066 - val_loss: 1.7056 - val_accuracy: 0.3088\n",
      "Epoch 6/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7075 - accuracy: 0.3101"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 395s 588ms/step - loss: 1.7075 - accuracy: 0.3101 - val_loss: 1.6925 - val_accuracy: 0.3148\n",
      "Epoch 7/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7013 - accuracy: 0.3175"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 381s 566ms/step - loss: 1.7013 - accuracy: 0.3175 - val_loss: 1.6866 - val_accuracy: 0.3284\n",
      "Epoch 8/10\n",
      "673/673 [==============================] - 321s 477ms/step - loss: 1.6893 - accuracy: 0.3228 - val_loss: 1.6941 - val_accuracy: 0.3326\n",
      "Epoch 9/10\n",
      "673/673 [==============================] - 321s 477ms/step - loss: 1.6839 - accuracy: 0.3271 - val_loss: 1.7265 - val_accuracy: 0.2947\n",
      "Epoch 10/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.6807 - accuracy: 0.3271"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 385s 573ms/step - loss: 1.6807 - accuracy: 0.3271 - val_loss: 1.6656 - val_accuracy: 0.3396\n",
      "225/225 [==============================] - 79s 339ms/step\n",
      "The accuracy on test data is: 33.408%\n",
      "\n",
      "\n",
      "\n",
      "MODEL: 8 TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: \n",
      "hyperparameter currently being tested: lr [0.01]\n",
      "\tpooling value: avg\n",
      "\tbias_init value: <keras.initializers.initializers_v2.RandomUniform object at 0x7f934f204850>\n",
      "\tlr value: 0.01\n",
      "\toptimizer value: Adamax\n",
      "\tbatch_size value: 32\n",
      "\tactivation_function value: relu\n",
      "\tbias_regularizer value: l2\n",
      "\tdropout_rate value: 0.4\n",
      "\tloss_function value: <keras.losses.SparseCategoricalCrossentropy object at 0x7f934f204880>\n",
      "\tepoch value: 10\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense-1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense-2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense-3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense-4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,517,255\n",
      "Trainable params: 6,952,455\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.9682 - accuracy: 0.2493"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 431s 627ms/step - loss: 1.9682 - accuracy: 0.2493 - val_loss: 1.8173 - val_accuracy: 0.2456\n",
      "Epoch 2/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.8052 - accuracy: 0.2524"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 381s 566ms/step - loss: 1.8052 - accuracy: 0.2524 - val_loss: 1.7896 - val_accuracy: 0.2522\n",
      "Epoch 3/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7939 - accuracy: 0.2564"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 379s 563ms/step - loss: 1.7939 - accuracy: 0.2564 - val_loss: 1.7689 - val_accuracy: 0.2721\n",
      "Epoch 4/10\n",
      "673/673 [==============================] - 354s 526ms/step - loss: 1.7789 - accuracy: 0.2645 - val_loss: 1.7809 - val_accuracy: 0.2619\n",
      "Epoch 5/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7608 - accuracy: 0.2751"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 397s 591ms/step - loss: 1.7608 - accuracy: 0.2751 - val_loss: 1.7614 - val_accuracy: 0.2819\n",
      "Epoch 6/10\n",
      "673/673 [==============================] - 342s 509ms/step - loss: 1.7544 - accuracy: 0.2761 - val_loss: 1.7858 - val_accuracy: 0.2610\n",
      "Epoch 7/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7418 - accuracy: 0.2886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 386s 573ms/step - loss: 1.7418 - accuracy: 0.2886 - val_loss: 1.7165 - val_accuracy: 0.3090\n",
      "Epoch 8/10\n",
      "673/673 [==============================] - 333s 495ms/step - loss: 1.7293 - accuracy: 0.2975 - val_loss: 1.7214 - val_accuracy: 0.3000\n",
      "Epoch 9/10\n",
      "673/673 [==============================] - 358s 533ms/step - loss: 1.7283 - accuracy: 0.2967 - val_loss: 1.7176 - val_accuracy: 0.3131\n",
      "Epoch 10/10\n",
      "673/673 [==============================] - 324s 481ms/step - loss: 1.7262 - accuracy: 0.3009 - val_loss: 1.7317 - val_accuracy: 0.3160\n",
      "225/225 [==============================] - 82s 354ms/step\n",
      "The accuracy on test data is: 24.714%\n",
      "\n",
      "\n",
      "\n",
      "MODEL: 9 TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: \n",
      "hyperparameter currently being tested: lr [0.1]\n",
      "\tpooling value: avg\n",
      "\tbias_init value: <keras.initializers.initializers_v2.RandomUniform object at 0x7f934f204850>\n",
      "\tlr value: 0.1\n",
      "\toptimizer value: Adamax\n",
      "\tbatch_size value: 32\n",
      "\tactivation_function value: relu\n",
      "\tbias_regularizer value: l2\n",
      "\tdropout_rate value: 0.4\n",
      "\tloss_function value: <keras.losses.SparseCategoricalCrossentropy object at 0x7f934f204880>\n",
      "\tepoch value: 10\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense-1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense-2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense-3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense-4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,517,255\n",
      "Trainable params: 6,952,455\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 837.0463 - accuracy: 0.2463"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 422s 620ms/step - loss: 837.0463 - accuracy: 0.2463 - val_loss: 2.1430 - val_accuracy: 0.2456\n",
      "Epoch 2/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 2.1021 - accuracy: 0.2532"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 393s 585ms/step - loss: 2.1021 - accuracy: 0.2532 - val_loss: 2.0435 - val_accuracy: 0.2456\n",
      "Epoch 3/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 2.0023 - accuracy: 0.2531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 397s 590ms/step - loss: 2.0023 - accuracy: 0.2531 - val_loss: 1.9739 - val_accuracy: 0.2456\n",
      "Epoch 4/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.9580 - accuracy: 0.2529"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 439s 652ms/step - loss: 1.9580 - accuracy: 0.2529 - val_loss: 1.9270 - val_accuracy: 0.2456\n",
      "Epoch 5/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.8979 - accuracy: 0.2532"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 360s 535ms/step - loss: 1.8979 - accuracy: 0.2532 - val_loss: 1.8838 - val_accuracy: 0.2456\n",
      "Epoch 6/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.8586 - accuracy: 0.2523"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 357s 531ms/step - loss: 1.8586 - accuracy: 0.2523 - val_loss: 1.8497 - val_accuracy: 0.2456\n",
      "Epoch 7/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.8330 - accuracy: 0.2533"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 395s 586ms/step - loss: 1.8330 - accuracy: 0.2533 - val_loss: 1.8293 - val_accuracy: 0.2456\n",
      "Epoch 8/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.8153 - accuracy: 0.2533"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 409s 608ms/step - loss: 1.8153 - accuracy: 0.2533 - val_loss: 1.8179 - val_accuracy: 0.2456\n",
      "Epoch 9/10\n",
      "673/673 [==============================] - 353s 525ms/step - loss: 1.8146 - accuracy: 0.2532 - val_loss: 1.8205 - val_accuracy: 0.2456\n",
      "Epoch 10/10\n",
      "673/673 [==============================] - 322s 477ms/step - loss: 1.8139 - accuracy: 0.2531 - val_loss: 1.8190 - val_accuracy: 0.2456\n",
      "225/225 [==============================] - 83s 354ms/step\n",
      "The accuracy on test data is: 24.714%\n",
      "\n",
      "\n",
      "\n",
      "MODEL: 10 TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: \n",
      "hyperparameter currently being tested: optimizer [Adam]\n",
      "\tpooling value: avg\n",
      "\tbias_init value: <keras.initializers.initializers_v2.RandomUniform object at 0x7f934f204850>\n",
      "\tlr value: 0.01\n",
      "\toptimizer value: Adam\n",
      "\tbatch_size value: 32\n",
      "\tactivation_function value: relu\n",
      "\tbias_regularizer value: l2\n",
      "\tdropout_rate value: 0.4\n",
      "\tloss_function value: <keras.losses.SparseCategoricalCrossentropy object at 0x7f934f204880>\n",
      "\tepoch value: 10\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense-1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense-2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense-3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense-4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,517,255\n",
      "Trainable params: 6,952,455\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 2.0427 - accuracy: 0.2477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 425s 606ms/step - loss: 2.0427 - accuracy: 0.2477 - val_loss: 1.8225 - val_accuracy: 0.2456\n",
      "Epoch 2/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.8175 - accuracy: 0.2531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 389s 578ms/step - loss: 1.8175 - accuracy: 0.2531 - val_loss: 1.8202 - val_accuracy: 0.2456\n",
      "Epoch 3/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.8113 - accuracy: 0.2531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 480s 713ms/step - loss: 1.8113 - accuracy: 0.2531 - val_loss: 1.8175 - val_accuracy: 0.2456\n",
      "Epoch 4/10\n",
      "673/673 [==============================] - 338s 502ms/step - loss: 1.8108 - accuracy: 0.2532 - val_loss: 1.8176 - val_accuracy: 0.2456\n",
      "Epoch 5/10\n",
      "673/673 [==============================] - 296s 440ms/step - loss: 1.8109 - accuracy: 0.2533 - val_loss: 1.8203 - val_accuracy: 0.2456\n",
      "Epoch 6/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.8114 - accuracy: 0.2532"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 391s 582ms/step - loss: 1.8114 - accuracy: 0.2532 - val_loss: 1.8175 - val_accuracy: 0.2456\n",
      "Epoch 7/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.8120 - accuracy: 0.2531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 398s 592ms/step - loss: 1.8120 - accuracy: 0.2531 - val_loss: 1.8172 - val_accuracy: 0.2456\n",
      "Epoch 8/10\n",
      "673/673 [==============================] - 344s 512ms/step - loss: 1.8122 - accuracy: 0.2527 - val_loss: 1.8178 - val_accuracy: 0.2456\n",
      "Epoch 9/10\n",
      "673/673 [==============================] - 331s 492ms/step - loss: 1.8159 - accuracy: 0.2529 - val_loss: 1.8272 - val_accuracy: 0.2456\n",
      "Epoch 10/10\n",
      "673/673 [==============================] - 329s 488ms/step - loss: 1.8114 - accuracy: 0.2532 - val_loss: 1.8204 - val_accuracy: 0.2456\n",
      "225/225 [==============================] - 83s 358ms/step\n",
      "The accuracy on test data is: 27.278%\n",
      "\n",
      "\n",
      "\n",
      "MODEL: 11 TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: \n",
      "hyperparameter currently being tested: optimizer [Adamax]\n",
      "\tpooling value: avg\n",
      "\tbias_init value: <keras.initializers.initializers_v2.RandomUniform object at 0x7f934f204850>\n",
      "\tlr value: 0.01\n",
      "\toptimizer value: Adamax\n",
      "\tbatch_size value: 32\n",
      "\tactivation_function value: relu\n",
      "\tbias_regularizer value: l2\n",
      "\tdropout_rate value: 0.4\n",
      "\tloss_function value: <keras.losses.SparseCategoricalCrossentropy object at 0x7f934f204880>\n",
      "\tepoch value: 10\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_16 (InputLayer)       [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense-1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense-2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense-3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense-4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,517,255\n",
      "Trainable params: 6,952,455\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.9856 - accuracy: 0.2487"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 435s 633ms/step - loss: 1.9856 - accuracy: 0.2487 - val_loss: 1.8056 - val_accuracy: 0.2454\n",
      "Epoch 2/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7946 - accuracy: 0.2610"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 394s 586ms/step - loss: 1.7946 - accuracy: 0.2610 - val_loss: 1.7952 - val_accuracy: 0.2573\n",
      "Epoch 3/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7799 - accuracy: 0.2685"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 430s 639ms/step - loss: 1.7799 - accuracy: 0.2685 - val_loss: 1.7634 - val_accuracy: 0.2756\n",
      "Epoch 4/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7729 - accuracy: 0.2723"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 380s 564ms/step - loss: 1.7729 - accuracy: 0.2723 - val_loss: 1.7620 - val_accuracy: 0.2764\n",
      "Epoch 5/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7646 - accuracy: 0.2752"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 411s 611ms/step - loss: 1.7646 - accuracy: 0.2752 - val_loss: 1.7581 - val_accuracy: 0.2827\n",
      "Epoch 6/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7566 - accuracy: 0.2793"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 416s 618ms/step - loss: 1.7566 - accuracy: 0.2793 - val_loss: 1.7576 - val_accuracy: 0.2926\n",
      "Epoch 7/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7479 - accuracy: 0.2862"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 426s 634ms/step - loss: 1.7479 - accuracy: 0.2862 - val_loss: 1.7426 - val_accuracy: 0.2891\n",
      "Epoch 8/10\n",
      "673/673 [==============================] - 287s 426ms/step - loss: 1.7425 - accuracy: 0.2893 - val_loss: 1.7617 - val_accuracy: 0.2823\n",
      "Epoch 9/10\n",
      "673/673 [==============================] - 353s 524ms/step - loss: 1.7583 - accuracy: 0.2792 - val_loss: 1.7558 - val_accuracy: 0.2842\n",
      "Epoch 10/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7352 - accuracy: 0.2877"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 392s 583ms/step - loss: 1.7352 - accuracy: 0.2877 - val_loss: 1.7156 - val_accuracy: 0.2940\n",
      "225/225 [==============================] - 80s 345ms/step\n",
      "The accuracy on test data is: 29.284%\n",
      "\n",
      "\n",
      "\n",
      "MODEL: 12 TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: \n",
      "hyperparameter currently being tested: batch_size [16]\n",
      "\tpooling value: avg\n",
      "\tbias_init value: <keras.initializers.initializers_v2.RandomUniform object at 0x7f934f204850>\n",
      "\tlr value: 0.01\n",
      "\toptimizer value: Adamax\n",
      "\tbatch_size value: 16\n",
      "\tactivation_function value: relu\n",
      "\tbias_regularizer value: l2\n",
      "\tdropout_rate value: 0.4\n",
      "\tloss_function value: <keras.losses.SparseCategoricalCrossentropy object at 0x7f934f204880>\n",
      "\tepoch value: 10\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense-1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense-2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense-3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense-4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,517,255\n",
      "Trainable params: 6,952,455\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1346/1346 [==============================] - ETA: 0s - loss: 1.8980 - accuracy: 0.2490"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346/1346 [==============================] - 517s 378ms/step - loss: 1.8980 - accuracy: 0.2490 - val_loss: 1.8213 - val_accuracy: 0.2456\n",
      "Epoch 2/10\n",
      "1346/1346 [==============================] - ETA: 0s - loss: 1.8149 - accuracy: 0.2521"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346/1346 [==============================] - 532s 395ms/step - loss: 1.8149 - accuracy: 0.2521 - val_loss: 1.8202 - val_accuracy: 0.2456\n",
      "Epoch 3/10\n",
      "1346/1346 [==============================] - ETA: 0s - loss: 1.8117 - accuracy: 0.2524"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346/1346 [==============================] - 505s 375ms/step - loss: 1.8117 - accuracy: 0.2524 - val_loss: 1.8142 - val_accuracy: 0.2456\n",
      "Epoch 4/10\n",
      "1346/1346 [==============================] - ETA: 0s - loss: 1.8037 - accuracy: 0.2522"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346/1346 [==============================] - 525s 390ms/step - loss: 1.8037 - accuracy: 0.2522 - val_loss: 1.7972 - val_accuracy: 0.2456\n",
      "Epoch 5/10\n",
      "1346/1346 [==============================] - ETA: 0s - loss: 1.7922 - accuracy: 0.2585"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346/1346 [==============================] - 560s 416ms/step - loss: 1.7922 - accuracy: 0.2585 - val_loss: 1.7693 - val_accuracy: 0.2698\n",
      "Epoch 6/10\n",
      "1346/1346 [==============================] - 444s 330ms/step - loss: 1.7797 - accuracy: 0.2664 - val_loss: 1.7797 - val_accuracy: 0.2693\n",
      "Epoch 7/10\n",
      "1346/1346 [==============================] - 456s 339ms/step - loss: 1.7729 - accuracy: 0.2733 - val_loss: 1.7703 - val_accuracy: 0.2748\n",
      "Epoch 8/10\n",
      "1346/1346 [==============================] - 427s 317ms/step - loss: 1.7713 - accuracy: 0.2746 - val_loss: 1.7784 - val_accuracy: 0.2721\n",
      "Epoch 9/10\n",
      "1346/1346 [==============================] - ETA: 0s - loss: 1.7633 - accuracy: 0.2770"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346/1346 [==============================] - 607s 451ms/step - loss: 1.7633 - accuracy: 0.2770 - val_loss: 1.7562 - val_accuracy: 0.2788\n",
      "Epoch 10/10\n",
      "1346/1346 [==============================] - ETA: 0s - loss: 1.7619 - accuracy: 0.2777"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346/1346 [==============================] - 579s 430ms/step - loss: 1.7619 - accuracy: 0.2777 - val_loss: 1.7448 - val_accuracy: 0.2830\n",
      "225/225 [==============================] - 108s 464ms/step\n",
      "The accuracy on test data is: 27.389%\n",
      "\n",
      "\n",
      "\n",
      "MODEL: 13 TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: \n",
      "hyperparameter currently being tested: batch_size [32]\n",
      "\tpooling value: avg\n",
      "\tbias_init value: <keras.initializers.initializers_v2.RandomUniform object at 0x7f934f204850>\n",
      "\tlr value: 0.01\n",
      "\toptimizer value: Adamax\n",
      "\tbatch_size value: 32\n",
      "\tactivation_function value: relu\n",
      "\tbias_regularizer value: l2\n",
      "\tdropout_rate value: 0.4\n",
      "\tloss_function value: <keras.losses.SparseCategoricalCrossentropy object at 0x7f934f204880>\n",
      "\tepoch value: 10\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense-1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense-2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense-3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense-4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,517,255\n",
      "Trainable params: 6,952,455\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 2.0840 - accuracy: 0.2477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 620s 901ms/step - loss: 2.0840 - accuracy: 0.2477 - val_loss: 1.8283 - val_accuracy: 0.2456\n",
      "Epoch 2/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.8079 - accuracy: 0.2546"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 499s 741ms/step - loss: 1.8079 - accuracy: 0.2546 - val_loss: 1.7899 - val_accuracy: 0.2596\n",
      "Epoch 3/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7812 - accuracy: 0.2665"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 491s 729ms/step - loss: 1.7812 - accuracy: 0.2665 - val_loss: 1.7707 - val_accuracy: 0.2837\n",
      "Epoch 4/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7627 - accuracy: 0.2746"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 452s 673ms/step - loss: 1.7627 - accuracy: 0.2746 - val_loss: 1.7618 - val_accuracy: 0.2658\n",
      "Epoch 5/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7532 - accuracy: 0.2820"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 518s 770ms/step - loss: 1.7532 - accuracy: 0.2820 - val_loss: 1.7166 - val_accuracy: 0.3047\n",
      "Epoch 6/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7385 - accuracy: 0.2914"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 421s 626ms/step - loss: 1.7385 - accuracy: 0.2914 - val_loss: 1.7097 - val_accuracy: 0.3178\n",
      "Epoch 7/10\n",
      "673/673 [==============================] - 352s 523ms/step - loss: 1.7308 - accuracy: 0.2985 - val_loss: 1.7212 - val_accuracy: 0.3033\n",
      "Epoch 8/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7308 - accuracy: 0.3014"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 453s 673ms/step - loss: 1.7308 - accuracy: 0.3014 - val_loss: 1.7035 - val_accuracy: 0.3189\n",
      "Epoch 9/10\n",
      "673/673 [==============================] - 345s 513ms/step - loss: 1.7207 - accuracy: 0.3031 - val_loss: 1.7104 - val_accuracy: 0.3127\n",
      "Epoch 10/10\n",
      "673/673 [==============================] - ETA: 0s - loss: 1.7177 - accuracy: 0.3088"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673/673 [==============================] - 479s 712ms/step - loss: 1.7177 - accuracy: 0.3088 - val_loss: 1.6993 - val_accuracy: 0.3205\n",
      "225/225 [==============================] - 62s 267ms/step\n",
      "The accuracy on test data is: 32.432%\n",
      "\n",
      "\n",
      "\n",
      "MODEL: 14 TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: \n",
      "hyperparameter currently being tested: batch_size [64]\n",
      "\tpooling value: avg\n",
      "\tbias_init value: <keras.initializers.initializers_v2.RandomUniform object at 0x7f934f204850>\n",
      "\tlr value: 0.01\n",
      "\toptimizer value: Adamax\n",
      "\tbatch_size value: 64\n",
      "\tactivation_function value: relu\n",
      "\tbias_regularizer value: l2\n",
      "\tdropout_rate value: 0.4\n",
      "\tloss_function value: <keras.losses.SparseCategoricalCrossentropy object at 0x7f934f204880>\n",
      "\tepoch value: 10\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense-1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense-2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense-3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense-4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,517,255\n",
      "Trainable params: 6,952,455\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "337/337 [==============================] - ETA: 0s - loss: 2.0876 - accuracy: 0.2459"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 415s 1s/step - loss: 2.0876 - accuracy: 0.2459 - val_loss: 1.8223 - val_accuracy: 0.2456\n",
      "Epoch 2/10\n",
      "337/337 [==============================] - ETA: 0s - loss: 1.8106 - accuracy: 0.2520"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 400s 1s/step - loss: 1.8106 - accuracy: 0.2520 - val_loss: 1.8003 - val_accuracy: 0.2459\n",
      "Epoch 3/10\n",
      "337/337 [==============================] - ETA: 0s - loss: 1.7971 - accuracy: 0.2535"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 381s 1s/step - loss: 1.7971 - accuracy: 0.2535 - val_loss: 1.7900 - val_accuracy: 0.2491\n",
      "Epoch 4/10\n",
      "337/337 [==============================] - ETA: 0s - loss: 1.7791 - accuracy: 0.2636"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 463s 1s/step - loss: 1.7791 - accuracy: 0.2636 - val_loss: 1.7568 - val_accuracy: 0.2717\n",
      "Epoch 5/10\n",
      "337/337 [==============================] - ETA: 0s - loss: 1.7631 - accuracy: 0.2734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 437s 1s/step - loss: 1.7631 - accuracy: 0.2734 - val_loss: 1.7439 - val_accuracy: 0.2994\n",
      "Epoch 6/10\n",
      "337/337 [==============================] - ETA: 0s - loss: 1.7455 - accuracy: 0.2854"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 373s 1s/step - loss: 1.7455 - accuracy: 0.2854 - val_loss: 1.7207 - val_accuracy: 0.2920\n",
      "Epoch 7/10\n",
      "337/337 [==============================] - 341s 1s/step - loss: 1.7341 - accuracy: 0.2924 - val_loss: 1.7275 - val_accuracy: 0.2901\n",
      "Epoch 8/10\n",
      "337/337 [==============================] - 322s 951ms/step - loss: 1.7240 - accuracy: 0.2961 - val_loss: 1.7364 - val_accuracy: 0.2905\n",
      "Epoch 9/10\n",
      "337/337 [==============================] - 306s 909ms/step - loss: 1.7194 - accuracy: 0.3015 - val_loss: 1.7225 - val_accuracy: 0.3021\n",
      "Epoch 10/10\n",
      "337/337 [==============================] - ETA: 0s - loss: 1.7158 - accuracy: 0.3033"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337/337 [==============================] - 398s 1s/step - loss: 1.7158 - accuracy: 0.3033 - val_loss: 1.7201 - val_accuracy: 0.2909\n",
      "225/225 [==============================] - 100s 424ms/step\n",
      "The accuracy on test data is: 29.075%\n",
      "\n",
      "\n",
      "\n",
      "MODEL: 15 TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: \n",
      "hyperparameter currently being tested: batch_size [128]\n",
      "\tpooling value: avg\n",
      "\tbias_init value: <keras.initializers.initializers_v2.RandomUniform object at 0x7f934f204850>\n",
      "\tlr value: 0.01\n",
      "\toptimizer value: Adamax\n",
      "\tbatch_size value: 128\n",
      "\tactivation_function value: relu\n",
      "\tbias_regularizer value: l2\n",
      "\tdropout_rate value: 0.4\n",
      "\tloss_function value: <keras.losses.SparseCategoricalCrossentropy object at 0x7f934f204880>\n",
      "\tepoch value: 10\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_24 (InputLayer)       [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense-1 (Dense)             (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense-2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense-3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense-4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,517,255\n",
      "Trainable params: 6,952,455\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "169/169 [==============================] - ETA: 0s - loss: 2.3016 - accuracy: 0.2433"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 390s 2s/step - loss: 2.3016 - accuracy: 0.2433 - val_loss: 1.8167 - val_accuracy: 0.2456\n",
      "Epoch 2/10\n",
      "169/169 [==============================] - ETA: 0s - loss: 1.8041 - accuracy: 0.2540"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 358s 2s/step - loss: 1.8041 - accuracy: 0.2540 - val_loss: 1.7843 - val_accuracy: 0.2689\n",
      "Epoch 3/10\n",
      "169/169 [==============================] - ETA: 0s - loss: 1.7695 - accuracy: 0.2715"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 496s 3s/step - loss: 1.7695 - accuracy: 0.2715 - val_loss: 1.7535 - val_accuracy: 0.2728\n",
      "Epoch 4/10\n",
      "169/169 [==============================] - ETA: 0s - loss: 1.7527 - accuracy: 0.2810"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 471s 3s/step - loss: 1.7527 - accuracy: 0.2810 - val_loss: 1.7415 - val_accuracy: 0.2862\n",
      "Epoch 5/10\n",
      "169/169 [==============================] - ETA: 0s - loss: 1.7466 - accuracy: 0.2868"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "###           MULTIPLE Hyperparameters to Be Tested               ###\n",
    "#####################################################################\n",
    "MODEL_NUMBER = 1\n",
    "FILENAME = 'results.csv'\n",
    "PATH_TO_DIR_CONTAINING_PLOTS = os.getcwd() + '/plots'\n",
    "PATH_TO_FILE_CONTAINING_RESULTS = ''\n",
    "I = 1\n",
    "# Enter Model Integer to Start At\n",
    "START_I = 4\n",
    "# for google colab;\n",
    "# FILENAME = '/content/drive/MyDrive/UCI/3_Junior/3_Spring_23/CS_172B/results.csv'\n",
    "# PATH_TO_DIR_CONTAINING_PLOTS = '/content/drive/MyDrive/UCI/3_Junior/3_Spring_23/CS_172B'\n",
    "# PATH_TO_FILE_CONTAINING_RESULTS = '/content/drive/MyDrive/UCI/3_Junior/3_Spring_23/CS_172B/'\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "# Note: for optimizer, just write if statement or create simple function\n",
    "#           to create the optimizer object based on str value\n",
    "hyperparam_pooling = ['avg', 'max']\n",
    "hyperparam_bias_init = [RandomUniform(minval=-0.01, maxval=0.01, seed=seed),\n",
    "                        Zeros(), HeNormal(seed=seed), HeUniform(seed=seed)]\n",
    "hyperparam_lr = [0.001, 0.01, 0.1]\n",
    "hyperparam_optimizer = ['Adam', 'Adamax']\n",
    "hyperparam_batch_size = [16, 32, 64, 128, 256, 512]\n",
    "hyperparam_activation_function = ['relu']\n",
    "hyperparam_bias_regularizer = ['l2']\n",
    "hyperparam_dropout_rate = [0.4, 0.3, 0.2]   # if using dropout layer(s)\n",
    "hyperparam_loss_function = [tf.keras.losses.SparseCategoricalCrossentropy()]\n",
    "hyperparam_epoch = [10, 25, 50, 100, 250]\n",
    "\n",
    "hyperparameters = {\n",
    "  # note: avg pooling returns a 2D tensor...idk wtf that is but probs need to resize somewhere\n",
    "  'pooling': hyperparam_pooling,\n",
    "  'bias_init': hyperparam_bias_init,\n",
    "  'lr': hyperparam_lr,\n",
    "  'optimizer': hyperparam_optimizer,\n",
    "  'batch_size': hyperparam_batch_size,\n",
    "  'activation_function': hyperparam_activation_function,\n",
    "  'bias_regularizer': hyperparam_bias_regularizer,\n",
    "  'dropout_rate': hyperparam_dropout_rate,\n",
    "  'loss_function': hyperparam_loss_function,\n",
    "  'epoch': hyperparam_epoch,\n",
    "}\n",
    "\n",
    "# CURRENT TESTING: want to test each hyperparameter 1 by 1 first,\n",
    "#   so we set everything to default at the start of each run for now,\n",
    "#   changing one hyperparamter at a time\n",
    "# defaults\n",
    "default_pooling = 'avg'\n",
    "default_bias_init = RandomUniform(minval=-0.01, maxval=0.01, seed=seed)\n",
    "default_lr = 0.01\n",
    "default_optimizer = 'Adamax'\n",
    "default_batch_size = 32\n",
    "default_activation_function = 'relu'\n",
    "default_bias_regularizer = 'l2'\n",
    "default_dropout_rate = 0.4\n",
    "default_loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "default_epoch = 10\n",
    "\n",
    "defaults = {\n",
    "    'pooling': default_pooling,\n",
    "    'bias_init': default_bias_init,\n",
    "    'lr': default_lr,\n",
    "    'optimizer': default_optimizer,\n",
    "    'batch_size': default_batch_size,\n",
    "    'activation_function': default_activation_function,\n",
    "    'bias_regularizer': default_bias_regularizer,\n",
    "    'dropout_rate': default_dropout_rate,\n",
    "    'loss_function': default_loss_function,\n",
    "    'epoch': default_epoch,\n",
    "}\n",
    "\n",
    "if START_I == 1:\n",
    "  # header for 'results.csv'\n",
    "  with open(FILENAME, 'a') as f:\n",
    "    f.write(\"Model No.,Pooling Type,Bias Init,Bias Regularizer,Optimizer,Activation Function,Loss Function,Dropout Rate,Learning Rate,Batch Size,Epoch,Train Accuracy,Validation Accuracy,Test Accuracy\\n\")\n",
    "\n",
    "\n",
    "# FOR LOOP STARTS HERE, EVERY HYPERPARAMETER SHOULD BE A VARIABLE\n",
    "for h_key, h_value in hyperparameters.items():\n",
    "  for h_element in h_value:\n",
    "    if I < START_I:\n",
    "      I += 1\n",
    "      MODEL_NUMBER += 1\n",
    "      continue\n",
    "    print(f'MODEL: {MODEL_NUMBER} TRAINING WITH THE FOLLOWING HYPER-PARAMETERS: ')\n",
    "    print(f'hyperparameter currently being tested: {h_key} [{h_element}]')\n",
    "\n",
    "    # set hyperparameter values\n",
    "    curr_pooling = defaults['pooling'] if h_key != 'pooling' else h_element\n",
    "    curr_bias_init = defaults['bias_init'] if h_key != 'bias_init' else h_element\n",
    "    curr_lr = defaults['lr'] if h_key != 'lr' else h_element\n",
    "    # NOTE: optimizer is still a string here!\n",
    "    curr_optimizer = defaults['optimizer'] if h_key != 'optimizer' else h_element\n",
    "    curr_batch_size = defaults['batch_size'] if h_key != 'batch_size' else h_element\n",
    "    curr_activation_function = defaults['activation_function'] if h_key != 'activation_function' else h_element\n",
    "    curr_bias_regularizer = defaults['bias_regularizer'] if h_key != 'bias_regularizer' else h_element\n",
    "    curr_dropout_rate = defaults['dropout_rate'] if h_key != 'dropout_rate' else h_element\n",
    "    curr_loss_function = defaults['loss_function'] if h_key != 'loss_function' else h_element\n",
    "    curr_epoch = defaults['epoch'] if h_key != 'epoch' else h_element\n",
    "\n",
    "    current = [\n",
    "      curr_pooling,\n",
    "      curr_bias_init,\n",
    "      curr_lr,\n",
    "      curr_optimizer,\n",
    "      curr_batch_size,\n",
    "      curr_activation_function,\n",
    "      curr_bias_regularizer,\n",
    "      curr_dropout_rate,\n",
    "      curr_loss_function,\n",
    "      curr_epoch\n",
    "    ]\n",
    "\n",
    "    for i, key in enumerate(hyperparameters.keys()):\n",
    "      print(f'\\t{key} value: {current[i]}')\n",
    "\n",
    "\n",
    "    #####################################################################\n",
    "    ###                    Load ResNet base_model                     ###\n",
    "    #####################################################################\n",
    "    base_model = ResNet50V2(include_top=False, \n",
    "                            weights='imagenet', \n",
    "                            input_shape=(48, 48, 3), \n",
    "                            pooling=curr_pooling)\n",
    "\n",
    "    # freeze weights of base_model - note: this works because the 'trainable' attribute is recursive \n",
    "    # (i.e. bc we freeze the base model, every layer in the model is frozen)\n",
    "    base_model.trainable = False\n",
    "    # for layer in base_model.layers:\n",
    "    #     layer.trainable = False\n",
    "\n",
    "\n",
    "    #####################################################################\n",
    "    ###                         Create Model                          ###\n",
    "    #####################################################################\n",
    "    inputs = keras.Input(shape=(48, 48, 3))\n",
    "\n",
    "    # We make sure that the base_model is running in inference mode here,\n",
    "    # by passing `training=False`. This is important for fine-tuning later\n",
    "    x = base_model(\n",
    "          inputs, \n",
    "          training=False\n",
    "        )\n",
    "\n",
    "    x = Dense(\n",
    "          name='dense-1',\n",
    "          units=2048, \n",
    "          activation=curr_activation_function, \n",
    "          use_bias=True, \n",
    "          bias_regularizer=curr_bias_regularizer, \n",
    "          bias_initializer=curr_bias_init\n",
    "        )(x)\n",
    "    x = Dense(\n",
    "          name='dense-2',\n",
    "          units=1024, \n",
    "          activation=curr_activation_function, \n",
    "          use_bias=True, \n",
    "          bias_regularizer=curr_bias_regularizer, \n",
    "          bias_initializer=curr_bias_init\n",
    "        )(x)\n",
    "    x = Dense(\n",
    "          name='dense-3',\n",
    "          units=512, \n",
    "          activation=curr_activation_function, \n",
    "          use_bias=True, \n",
    "          bias_regularizer=curr_bias_regularizer, \n",
    "          bias_initializer=curr_bias_init\n",
    "        )(x)\n",
    "    # prevent overfitting & improve the generalization performance of the model by randomly \"dropping out\" \n",
    "    #   (i.e., temporarily removing) a portion of the units or nodes in a layer during training\n",
    "    x = Dropout(\n",
    "          name='dropout',\n",
    "          rate=curr_dropout_rate\n",
    "        )(x)\n",
    "    x = Dense(\n",
    "          name='dense-4',\n",
    "          units=256, \n",
    "          activation=curr_activation_function, \n",
    "          use_bias=True, \n",
    "          bias_regularizer=curr_bias_regularizer, \n",
    "          bias_initializer=curr_bias_init\n",
    "        )(x)\n",
    "    # only flatten if using conv2d lol...\n",
    "    # x = Flatten()(x)\n",
    "    outputs = Dense(\n",
    "          name='dense_output', \n",
    "          units=7, \n",
    "          activation='softmax'\n",
    "        )(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    #####################################################################\n",
    "    ###                        Compile Model                          ###\n",
    "    #####################################################################\n",
    "    model.compile(\n",
    "        loss = curr_loss_function, \n",
    "        optimizer= Adamax(learning_rate=curr_lr) if curr_optimizer == 'Adamax' else Adam(learning_rate=curr_lr), \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath = 'models/test', \n",
    "            save_freq = 'epoch',\n",
    "            save_best_only = True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    #####################################################################\n",
    "    ###                          Fit Model                            ###\n",
    "    #####################################################################\n",
    "    history = model.fit(\n",
    "        X_train, Y_train, \n",
    "        validation_data=(X_validation, Y_validation),\n",
    "        batch_size=curr_batch_size, \n",
    "        epochs=curr_epoch, \n",
    "        callbacks = callbacks\n",
    "    )\n",
    "\n",
    "\n",
    "    #####################################################################\n",
    "    ###                    Plot Training Results                      ###\n",
    "    ### Note: Modify code to store plots in a excel spreadsheet       ###\n",
    "    ###         rather than plot.                                     ###\n",
    "    #####################################################################\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(curr_epoch), acc, label='Training Accuracy')\n",
    "    plt.plot(range(curr_epoch), val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.savefig(f\"{PATH_TO_DIR_CONTAINING_PLOTS}/accuracy/training_validation_accuracy{MODEL_NUMBER}.png\")\n",
    "    plt.clf()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(curr_epoch), loss, label='Training Loss')\n",
    "    plt.plot(range(curr_epoch), val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig(f\"{PATH_TO_DIR_CONTAINING_PLOTS}/loss/training_validation_loss{MODEL_NUMBER}.png\")\n",
    "\n",
    "\n",
    "    #####################################################################\n",
    "    ###              Best Model Prediction on Test Data               ###\n",
    "    ### Note: write best model's prediction in new 'results.csv' file ###\n",
    "    #####################################################################\n",
    "    del(model)\n",
    "    model = load_model('models/test')\n",
    "    prediction = model.predict(X_test) # for each datapoint, gives a (1,7) vector contains probabilities\n",
    "\n",
    "    Y_prediction = np.argmax(prediction,axis = 1) # get the index(label) of highest probability for each \n",
    "    prediction_accuracy = np.mean(Y_prediction==Y_test)\n",
    "    print(f\"The accuracy on test data is: {prediction_accuracy*100:0.3f}%\")\n",
    "\n",
    "    with open(PATH_TO_FILE_CONTAINING_RESULTS + FILENAME, 'a') as f:\n",
    "      f.write(str(MODEL_NUMBER) + ',' +\n",
    "              str(curr_pooling) + ',' +\n",
    "              str(curr_bias_init) + ',' +\n",
    "              str(curr_bias_regularizer) + ',' +\n",
    "              str(curr_optimizer) + ',' +\n",
    "              str(curr_activation_function) + ',' +\n",
    "              str(curr_loss_function) + ',' +\n",
    "              str(curr_dropout_rate) + ',' +\n",
    "              str(curr_lr) + ',' +\n",
    "              str(curr_batch_size) + ',' +\n",
    "              str(curr_epoch) + ',' +\n",
    "              \"{:.4f}\".format(max(acc)) + ',' +\n",
    "              \"{:.4f}\".format(max(val_acc)) + ',' +\n",
    "              \"{:.4f}\".format(prediction_accuracy) + '\\n')\n",
    "\n",
    "\n",
    "    print('\\n\\n')\n",
    "    MODEL_NUMBER += 1\n",
    "# FOR LOOP ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-172b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
